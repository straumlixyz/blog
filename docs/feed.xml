<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/blog/" rel="alternate" type="text/html" /><updated>2023-12-23T18:04:03+02:00</updated><id>http://localhost:4000/blog/feed.xml</id><title type="html">Your awesome title</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">Evaluate Dangerous Capabilities Today</title><link href="http://localhost:4000/blog/console-launch" rel="alternate" type="text/html" title="Evaluate Dangerous Capabilities Today" /><published>2023-12-27T00:00:00+02:00</published><updated>2023-12-27T00:00:00+02:00</updated><id>http://localhost:4000/blog/console-launch</id><content type="html" xml:base="http://localhost:4000/blog/console-launch"><![CDATA[<p>After months of passionate iteration, we’re thrilled to launch our brand new <a href="https://console.straumli.ai/">console</a> that enables users to run state-of-the-art evaluations on their generative models with the click of a button.</p>

<p><img class="shadow-2xl rounded-md w-screen-lg pt-4" src="/blog/assets/console_launcheval.jpg" /></p>
<div class="text-center text-xs text-gray-400">Launching a new evaluation.</div>
<div class="pt-4"></div>

<p>This early version focuses heavily on surfacing misuse risks, helping organizations screen their models for dangerous capabilities before deploying them in the wild and allowing bad actors to exploit them. The current console ships with access to four evaluations targeting biowarfare and cyberwarfare, three of which are the first of their kind:</p>

<ul>
  <li><strong>Biowarfare 101</strong> enables organizations to asses their models’ knowledge of bioweapon design and deployment. While enhanced documentation is underway, we are keen on stressing the fact that we can successfully administer this evaluation <em>without us knowing the right answers</em>, thus leaving only the domain experts in the know. This is made possible by our recent <a href="/blog/introducing-hashmarks">cryptographic protocol</a>, paving the way for zero-trust safety infrastructure.</li>
  <li><strong>AutoHack</strong> enables organizations to assess their models’ ability to identify cybersecurity vulnerabilities in the context of dozens of whitebox “Capture The Flag” challenges. Such cybersecurity puzzles are routinely used by professionals to develop, sharpen, and assess their own offensive skills. Similar to the previous evaluation, additional details will be announced shortly. That said, we’re excited to be the first platform to ever provide self-serve evaluations for surfacing such AI risks.</li>
  <li><strong>Complicity</strong> complements the above two evaluations by measuring a model’s general openness to facilitating or engaging in illicit activities on behalf of the user. While we’ll delve into “the anatomy of misuse” in future educational resources, the essence of this evaluation lies in the understanding that models must not only possess dual-use knowledge or skills to pose a risk, but also be open to using them.</li>
  <li><strong>Model Leakage</strong> targets models deployed through user-facing endpoints, and assesses the traditional cybersecurity of such infrastructure. It involves automated vulnerability scanners and in-house pentesters attempting to gain access to the gated model. Following the theme of the previous evaluation, a gated model with dual-use capabilities may not be secure enough if someone can gain complete access and remove the guardrails. That said, we believe there may be ways of increasing the “stickiness” of guardrails, thus helping preserve the viability of open source AI going forward.</li>
</ul>

<p><img class="shadow-2xl rounded-md w-screen-lg pt-4" src="/blog/assets/console_registerartifact.jpg" /></p>
<div class="text-center text-xs text-gray-400">Registering a new artifact.</div>
<div class="pt-4"></div>

<p>While we’re engineers at heart, we believe anyone should be able to gain clarity on the available means of ensuring safe AI development. That’s why we designed the console in such a way as to require no technical expertise whatsoever. Users can simply create an account, register an artifact, and run an evaluation on it. Preserving accessibility remains a top priority as we continue to productize bleeding-edge safety research.</p>

<p>These four inaugural evaluations equip organizations with the means necessary to gain insight into the misuse potential of their systems today. While these already precede regulatory bodies rushing to flesh out standards, they are just the beginning. Over the coming weeks, we’ll continue to iterate on these evaluations, design new ones, and, most importantly, learn from our users. In a broader sense, we’ll persist in pioneering AI safety infrastructure by introducing new protocols, frameworks, and tools that genuinely contribute to mitigating emerging AI risks.</p>

<p><em>Are you involved in developing or deploying generative models? <a href="https://console.straumli.ai/">Run an evaluation</a> or <a href="https://calendly.com/paul-from-straumli/demo">book a demo</a> to understand how we can set you up for success in tomorrow’s regulatory environment by mitigating risks today.</em></p>]]></content><author><name></name></author><category term="product" /><summary type="html"><![CDATA[After months of passionate iteration, we’re thrilled to launch our brand new console that enables users to run state-of-the-art evaluations on their generative models with the click of a button.]]></summary></entry><entry><title type="html">Introducing Hashmarks</title><link href="http://localhost:4000/blog/introducing-hashmarks" rel="alternate" type="text/html" title="Introducing Hashmarks" /><published>2023-12-01T00:00:00+02:00</published><updated>2023-12-01T00:00:00+02:00</updated><id>http://localhost:4000/blog/introducing-hashmarks</id><content type="html" xml:base="http://localhost:4000/blog/introducing-hashmarks"><![CDATA[<p>How can we assess dangerous capabilities without disclosing sensitive information? Traditional benchmarks are like exams for AI, complete with reference solutions. However, a benchmark on bioterrorism would amount to a public FAQ on a sensitive topic.</p>

<p>To enable evaluation while mitigating misuse, we introduce <a href="https://arxiv.org/abs/2312.00645">hashmarks</a>, a simple alternative to benchmarks. In their basic form, hashmarks are benchmarks whose reference solutions have been cryptographically hashed prior to publication.</p>

<p>To <a href="https://arxiv.org/pdf/2312.00645.pdf#page=3">assess performance on a hashmark</a>, developers first get their AI to answer an exam question. Then, they hash the candidate answer and see whether the hash matches the reference one. If the model got it wrong, the correct answer remains secret.</p>

<p>However, things are not that simple. We investigate <a href="https://arxiv.org/pdf/2312.00645.pdf#page=5">the resilience of hashmarks</a> against half a dozen failure modes, ranging from rainbow table attacks to the Streisand effects associated with obfuscating sensitive information.</p>

<p>All in all, hashmarks provide just one tool in our growing arsenal of high-stakes AI evaluations. We look forward to engaging with community feedback before pushing forward with concrete instances of hashmarks.</p>]]></content><author><name></name></author><category term="research" /><summary type="html"><![CDATA[How can we assess dangerous capabilities without disclosing sensitive information? Traditional benchmarks are like exams for AI, complete with reference solutions. However, a benchmark on bioterrorism would amount to a public FAQ on a sensitive topic.]]></summary></entry></feed>