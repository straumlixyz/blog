<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="/blog/" rel="alternate" type="text/html" /><updated>2023-12-10T19:50:24+02:00</updated><id>/blog/feed.xml</id><title type="html">Your awesome title</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">Introducing Hashmarks</title><link href="/blog/2023/12/01/introducing-hashmarks.html" rel="alternate" type="text/html" title="Introducing Hashmarks" /><published>2023-12-01T00:00:00+02:00</published><updated>2023-12-01T00:00:00+02:00</updated><id>/blog/2023/12/01/introducing-hashmarks</id><content type="html" xml:base="/blog/2023/12/01/introducing-hashmarks.html"><![CDATA[<p>How can we assess dangerous capabilities without disclosing sensitive information? Traditional benchmarks are like exams for AI, complete with reference solutions. However, a benchmark on bioterrorism would amount to a public FAQ on a sensitive topic.</p>

<p>To enable evaluation while mitigating misuse, we introduce <a href="https://arxiv.org/abs/2312.00645">hashmarks</a>, a simple alternative to benchmarks. In their basic form, hashmarks are benchmarks whose reference solutions have been cryptographically hashed prior to publication.</p>

<p>To <a href="https://arxiv.org/pdf/2312.00645.pdf#page=3">assess performance on a hashmark</a>, developers first get their AI to answer an exam question. Then, they hash the candidate answer and see whether the hash matches the reference one. If the model got it wrong, the correct answer remains secret.</p>

<p>However, things are not that simple. We investigate <a href="https://arxiv.org/pdf/2312.00645.pdf#page=5">the resilience of hashmarks</a> against half a dozen failure modes, ranging from rainbow table attacks to the Streisand effects associated with obfuscating sensitive information.</p>

<p>All in all, hashmarks provide just one tool in our growing arsenal of high-stakes AI evaluations. We look forward to engaging with community feedback before pushing forward with concrete instances of hashmarks.</p>]]></content><author><name></name></author><category term="research" /><category term="evaluations" /><summary type="html"><![CDATA[How can we assess dangerous capabilities without disclosing sensitive information? Traditional benchmarks are like exams for AI, complete with reference solutions. However, a benchmark on bioterrorism would amount to a public FAQ on a sensitive topic.]]></summary></entry></feed>